# Learning rate, Data preprocessing, Overfitting

ğŸ…[Edwith](https://www.edwith.org/) - [ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ BASIC](https://www.edwith.org/others26/joinLectures/9829)

## Learning rateì˜ ì¤‘ìš”ì„±

- Stepì˜ ê°„ê²©ì´ ì ì ˆí•˜ì§€ ì•Šìœ¼ë©´, í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠìŒ.
- ë§¤ìš° í° ê°’ìœ¼ë¡œ ì„¤ì • ì‹œ, learning rate ìˆ«ìê°€ ì•„ë‹Œ ë§¤ìš° í° ê°’ ë“± ì¶œë ¥í•  ìˆ˜ ìˆê²ŒëŒ.
- ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì • ì‹œ, í•™ìŠµ ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¼.
- ì´ì™€ ê°™ì€ ê²½ìš°ë¥¼ **OverShooting** ì´ë¼ê³  ë§í•¨.

## Try several learning rates

- Observe the cost function
- Check it goes down in a reasonable rate
- 0.01 ë¶€í„° ì‹œì‘í•˜ì—¬, í•¨ìˆ˜ì˜ ê°’ì„ í™˜ì‚°í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ.

## Data (X) Preprocessing for Gradient descent

- Normalize data : cost í•¨ìˆ˜ê°€ ë°œì‚° í•œë‹¤ë˜ê°€, ë°ì´í„° ì°¨ì´ê°€ í° ê²½ìš° ì‚¬ìš©í•˜ê¸° ì¢‹ìŒ.
- zero-centered data : ìœ„ì™€ ê°™ì€ ì´ìœ 

## Standardization (Normalize data ì¤‘ í•˜ë‚˜)

- `X_std[:,0] = (X[:,0] - X[:, 0].mean()) / X[:,0].std()`

## Overfitting ì´ë€?

- Our model is very good with training data set(with memorization)
- Not good at test dataset or in real use
- ìš°ë¦¬ê°€ í…ŒìŠ¤íŠ¸ í•œ í•™ìŠµ ë°ì´í„°ë“¤ì´ êµ¬í˜„í•œ í•¨ìˆ˜ì— ë„ˆë¬´ ì•Œë§ê²Œ ì„¤ì •ë˜ì–´ì„œ ì‹¤ì œ ë°ì´í„°ê°’ì„ ëŒ€ì… ì‹œ, ì¼ì¹˜ í™•ë¥ ì´ ë–¨ì–´ì§€ëŠ” ê²½ìš° ì…ë‹ˆë‹¤.

## Solutions for overfitting

- More Training data! => ë§ì€ ë°ì´í„°ë“¤ì„ ì´ìš©í•´ ê²½ìš°ì˜ ìˆ˜ë¥¼ ë‹¤ì–‘í•˜ê²Œ íŒŒì•…í•´ì•¼í•©ë‹ˆë‹¤.
- Reduce the number of features => ì¤‘ë³µ ëœ ê°’ë“¤ì„ ì¤„ì…ë‹ˆë‹¤.
- **Regularization** (ì¼ë°˜í™”)

## Regularization

- Let's not have too big numbers in the weight
- weight ê°’ì´ ì ì€ ê°’, cost ê°’ì„ ìµœì†Œí™”
- ![image](https://user-images.githubusercontent.com/60251579/95007728-de7e4e00-064d-11eb-9690-29e8908ca742.png)
  - regularization strength
  - `l2reg = 0.001 * tf.reduce_sum(tf.square(W))`
  - SquareëŠ” ì œê³±ìˆ˜, 0.001ë¡œ ìµœì†Œí™” ì‹œí‚¨ë‹¤.

## Summary

1. **Learning rate**
2. **Data preprocessing**
3. **Overfitting** ì•„ì£¼ ì¤‘ìš”!!
   - More training data
   - Regularization
