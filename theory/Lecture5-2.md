# Logistic classification : Cost function & Gradient decent

ğŸ…[Edwith](https://www.edwith.org/) - [ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ BASIC](https://www.edwith.org/others26/joinLectures/9829)

## Cost Function

- H(x) = Wx + b ì˜ ê²½ìš°, ìµœì†Œ ê°’ì„ ì°¾ì•„ì•¼í•¨.

- ì„ í˜• íšŒê·€
  - H(x) = 1 -> cost = O
  - H(x) = 0 -> cost = ë¬´í•œ
  - H(x) = 0, cost = 0
  - h(x) = 1, cost = ë¬´í•œ

## Cost Function

- C(H(x), y) = - ylog(H(x)) - (1 - y)log(1 - H(x))
  - y = 1, c = -log(1+h(x))
  - y = 0, c = -log(1-h(x))

## Minimize cost - Gradient decent Algorithm

- ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ë¯¸ë¶„)
- W (GradientDescentOptimizer) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¡´ì¬.

```python
# cost function
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis)))
```

```python
# Minimize
a = tf.Variable(0.1) # Learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)

```

## Exercise

- CSV reading using tf.decode_csv
- Try other classification data from [Kaggle](https://www.kaggle.com/)
