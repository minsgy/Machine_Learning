# Linear Regressionì˜ Cost ìµœì†Œí™” ì•Œê³ ë¦¬ì¦˜ì˜ ì›ë¦¬

ğŸ…[Edwith](https://www.edwith.org/) - [ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ BASIC](https://www.edwith.org/others26/joinLectures/9829)

## Simplified Hypothesis

- H(x) = Wx
- Cost(W)

## What cost(W) looks like?

- ìµœì†Œí•œì˜ Wê°’ì„ ì°¾ì. minimize
- W = 1, cost(W) = 0

  - (1+1-1)^2 + (1*2-2) + (1*3-3) = 0

- W = 0, cost(W) = 4.67

  - 1/3((0*1-1)^2 + (0*2-2)^2 + (0\*3-3)^2) = 4.67

- W = 2, cost(W) = 4.67

- ![image](https://user-images.githubusercontent.com/60251579/94142973-0df0c600-feaa-11ea-84f3-5c41be3b6489.png)

  | **x** | **y** |
  | ----- | ----- |
  | 1     | 1     |
  | 2     | 2     |
  | 3     | 3     |

## Gradient descent algorithm - ê²½ì‚¬ì— ë”°ë¼ ë‚´ë ¤ê°€ëŠ” ì•Œê³ ë¦¬ì¦˜

- Minimize cost function
- Gradient descent is used many minimization problems
- For a given cost function, cost(W, b), it will find W, b to minimize cost
- It can be applied to more general function: cost(w1, w2, ...)

## How it works?

- Start with inital guesses

  - Start any other value or 0,0
  - Keeping changing `W` and `b` a little bit to try and reduce cost(W,b)

- Each time you change the parameters, you select the gradient which reduces cost(W,b) the most possible.
- Repeat
- Do so until you converge to a local minimum
- Has an interesting property
  - Where you start can determine which minimum you end up

## Formal definition

- [ë¯¸ë¶„ ê³„ì‚° WEB](https://www.derivative-calculator.net/)

## Gradient descent algorithm

-![image](https://user-images.githubusercontent.com/60251579/94146019-51e5ca00-feae-11ea-8ce8-5ebeeddaf8ad.png)

## Convex function

- cost(W, b)ë¥¼ ê±°ê¾¸ë¡œ ë’¤ì§‘ì–´ ë†“ì€ ê²½ìš°
- ì‹œì‘ì ë§ˆë‹¤ ê²°ê³¼ë¥¼ í™•ì • í•  ìˆ˜ ì—†ëŠ” ê²ƒì„ í™•ì • ì‹œí‚´.
- Gradient ì•Œê³ ë¦¬ì¦˜ì´ ë¬´ì¡°ê±´ ë‹µì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í™•ì¸.

## Gradient Descent Tensorflow êµ¬í˜„

![image](https://user-images.githubusercontent.com/60251579/94149856-611b4680-feb3-11ea-9780-f09ba4ccad54.png)

```python
# Minimize : Gradient Descent using derivative
# W -= Learning_rate(alpha)* derivative
learning_rate = 0.1 # í•™ìŠµë¥  ì„¤ì •
gradient = tf.reduce_mean((W*X-Y) * X) # ê³„ì‚° ìˆ˜ì‹
descent = W - learning_rate * gradient # W - gradient ê°’ ê³„ì‚°.
update = W.assign(descent) # ê²Œì‚°ê°’ assign í•´ì¤˜ì„œ ë„˜ê¹€
```

## Optimizerì„ í†µí•´ GradientDescent ì›í•˜ëŠ” ëŒ€ë¡œ ìˆ˜ì • ê°€ëŠ¥.

- ìœ„ ì‹ê³¼ ê°™ìŒ.
- `optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)`
